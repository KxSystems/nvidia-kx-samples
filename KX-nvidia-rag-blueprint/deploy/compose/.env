# ==========================
# User / Auth
# ==========================
export USERID=$(id -u)
export NVIDIA_API_KEY=${NGC_API_KEY}

# ==========================
# KDB.AI Vector Store (Default)
# ==========================
export APP_VECTORSTORE_NAME="kdbai"
export APP_VECTORSTORE_URL="http://kdbai:8082"
export APP_VECTORSTORE_SEARCHTYPE="dense"
export APP_VECTORSTORE_ENABLEGPUINDEX=True
export APP_VECTORSTORE_ENABLEGPUSEARCH=True
export KDBAI_DATABASE="default"
export KDBAI_INDEX_TYPE="cagra"
export KDBAI_API_KEY=""
export KDBAI_CAGRA_SINGLE_BATCH="false"

# ==========================
# On-Prem NIM Endpoints
# ==========================
export APP_LLM_SERVERURL=nim-llm:8000
export APP_FILTEREXPRESSIONGENERATOR_SERVERURL=nim-llm:8000
export SUMMARY_LLM_SERVERURL=nim-llm:8000
export APP_EMBEDDINGS_SERVERURL=nemoretriever-embedding-ms:8000
export APP_RANKING_SERVERURL=nemoretriever-ranking-ms:8000
export OCR_GRPC_ENDPOINT=paddle:8001
export OCR_INFER_PROTOCOL=grpc
export OCR_MODEL_NAME=paddle
export OCR_HTTP_ENDPOINT=http://paddle:8000/v1/infer
export YOLOX_GRPC_ENDPOINT=page-elements:8001
export YOLOX_INFER_PROTOCOL=grpc
export YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT=graphic-elements:8001
export YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=grpc
export YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT=table-structure:8001
export YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=grpc

# ==========================
# Cloud NIM Endpoints (optional)
# ==========================

# export APP_EMBEDDINGS_SERVERURL=""
# export APP_LLM_SERVERURL=""
# export APP_LLM_MODELNAME=nvidia/llama-3.3-nemotron-super-49b-v1.5
# export APP_FILTEREXPRESSIONGENERATOR_MODELNAME=nvidia/llama-3.3-nemotron-super-49b-v1.5
# export APP_FILTEREXPRESSIONGENERATOR_SERVERURL=""
# export SUMMARY_LLM="nvidia/llama-3.3-nemotron-super-49b-v1.5"
# export APP_RANKING_SERVERURL=""
# export SUMMARY_LLM_SERVERURL=""
# export OCR_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/baidu/paddleocr
# export OCR_INFER_PROTOCOL=http
# export OCR_MODEL_NAME=paddle
# export YOLOX_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2
# export YOLOX_INFER_PROTOCOL=http
# export YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1
# export YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=http
# export YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1
# export YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=http
# export APP_QUERYREWRITER_SERVERURL=""
# export APP_QUERYREWRITER_MODELNAME="nvidia/llama-3.3-nemotron-super-49b-v1.5"


# ==========================
# GPU IDs (local deployment)
# ==========================
# IMPORTANT: Customize these based on your GPU count and memory.
# The 49B LLM model requires 2 GPUs for tensor parallelism.
# Services sharing a GPU must fit in available memory together.
#
# Example for 4+ GPU system:
#   GPU 0,1: LLM (49B model needs ~98GB, requires 2 GPUs)
#   GPU 2: Embedding + Reranker (~12GB combined)
#   GPU 3: Ingestion NIMs + KDB.AI (~30GB combined)
#
# For 2-GPU systems, use cloud-hosted LLM endpoints instead.

# ==== LLM (requires 2 GPUs for 49B model) ====
export LLM_MS_GPU_ID=0,1
export LLM_GPU_COUNT=2

# ==== LLM Model Configuration ====
# Model profile - run 'docker compose -f nims.yaml run nim-llm list-model-profiles'
# For 2-GPU tensor parallelism, use tp2 profiles (e.g., vllm-bf16-tp2-pp1-...)
# export NIM_MODEL_PROFILE=""

# Maximum context length (default 131072 requires significant GPU memory)
# For RAG workloads, 32768 is usually sufficient
export NIM_MAX_MODEL_LEN=32768

# ==== Embeddings + Reranker (can share GPU, ~12GB combined) ====
export EMBEDDING_MS_GPU_ID=2
export VLM_EMBEDDING_MS_GPU_ID=2
export RANKING_MS_GPU_ID=2

# ==== Vector DB GPU ID ====
export VECTORSTORE_GPU_DEVICE_ID=3

# ==== Ingestion NIMs (can share GPU, ~25GB combined) ====
export YOLOX_MS_GPU_ID=3
export YOLOX_GRAPHICS_MS_GPU_ID=3
export YOLOX_TABLE_MS_GPU_ID=3
export OCR_MS_GPU_ID=3

# ==========================
# Paths
# ==========================

export PROMPT_CONFIG_FILE=${PWD}/src/nvidia_rag/rag_server/prompt.yaml