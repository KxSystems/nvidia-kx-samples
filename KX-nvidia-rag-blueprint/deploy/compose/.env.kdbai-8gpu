# ==========================
# KDB.AI Deployment - 8x RTX PRO 6000 Blackwell (96GB each)
# ==========================
#
# GPU Assignment:
#   GPU 0,1: LLM (49B model requires 2 GPUs for tensor parallelism - bf16 ~98GB)
#   GPU 2: Embedding + Reranker (shared)
#   GPU 3: Page Elements NIM
#   GPU 4: Graphic Elements NIM
#   GPU 5: Table Structure NIM + PaddleOCR (shared)
#   GPU 6: KDB.AI cuVS (GPU-accelerated vector DB)
#   GPU 7: VLM (multimodal)
#
# Usage:
#   1. Copy this file: cp .env.kdbai-8gpu .env.kdbai-8gpu.local
#   2. Edit .env.kdbai-8gpu.local with your credentials
#   3. source .env.kdbai-8gpu.local
#   4. Deploy: docker compose --env-file .env.kdbai-8gpu.local \
#              -f vectordb.yaml -f nims.yaml -f docker-compose-rag-server.yaml \
#              -f docker-compose-ingestor-server.yaml --profile kdbai up -d

# ==========================
# NVIDIA NGC API Key (REQUIRED)
# ==========================
export NGC_API_KEY="your-ngc-api-key"
export NVIDIA_API_KEY="${NGC_API_KEY}"

# ==========================
# KX Docker Registry Authentication (REQUIRED)
# ==========================
export KDBAI_REGISTRY_EMAIL="your-email@example.com"
export KDBAI_REGISTRY_TOKEN="your-bearer-token"

# ==========================
# KDB.AI License (REQUIRED)
# ==========================
export KDB_LICENSE_B64="your-kdb-license-b64-string"

# ==========================
# KDB.AI Server Settings
# ==========================
export KDBAI_THREADS=16
export KDBAI_ENDPOINT="http://kdbai:8082"
export KDBAI_API_KEY=""
export KDBAI_DATABASE="default"
export KDBAI_INDEX_TYPE="hnsw"

# ==========================
# Vector Store Configuration
# ==========================
export APP_VECTORSTORE_NAME="kdbai"
export APP_VECTORSTORE_URL="http://kdbai:8082"
export APP_VECTORSTORE_SEARCHTYPE="dense"

# GPU-accelerated indexing (set to True if using kdbai-gpu profile)
export APP_VECTORSTORE_ENABLEGPUINDEX=False
export APP_VECTORSTORE_ENABLEGPUSEARCH=False

# ==========================
# GPU ID Assignments (8x RTX PRO 6000 Blackwell - 96GB each)
# ==========================

# GPU 0,1: LLM (49B model requires 2 GPUs for tensor parallelism)
export LLM_GPU_COUNT=2

# GPU 2: Embedding + Reranker (shared)
export EMBEDDING_MS_GPU_ID=2
export RANKING_MS_GPU_ID=2

# GPU 3: Page Elements NIM
export YOLOX_MS_GPU_ID=3

# GPU 4: Graphic Elements NIM
export YOLOX_GRAPHICS_MS_GPU_ID=4

# GPU 5: Table Structure NIM + PaddleOCR (shared)
export YOLOX_TABLE_MS_GPU_ID=5
export OCR_MS_GPU_ID=5

# GPU 6: KDB.AI cuVS (GPU-accelerated vector search)
export KDBAI_GPU_DEVICE_ID=6

# GPU 7: VLM (multimodal support)
export VLM_MS_GPU_ID=7
export VLM_EMBEDDING_MS_GPU_ID=7

# ==========================
# NIM Endpoints (On-Prem)
# ==========================
export APP_LLM_SERVERURL=nim-llm:8000
export APP_FILTEREXPRESSIONGENERATOR_SERVERURL=nim-llm:8000
export SUMMARY_LLM_SERVERURL=nim-llm:8000
export APP_EMBEDDINGS_SERVERURL=nemoretriever-embedding-ms:8000
export APP_RANKING_SERVERURL=nemoretriever-ranking-ms:8000

# ==========================
# Ingestion NIM Endpoints
# ==========================
export OCR_GRPC_ENDPOINT=paddle:8001
export OCR_INFER_PROTOCOL=grpc
export OCR_MODEL_NAME=paddle
export OCR_HTTP_ENDPOINT=http://paddle:8000/v1/infer
export YOLOX_GRPC_ENDPOINT=page-elements:8001
export YOLOX_INFER_PROTOCOL=grpc
export YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT=graphic-elements:8001
export YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=grpc
export YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT=table-structure:8001
export YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=grpc

# ==========================
# User / Paths
# ==========================
export USERID=$(id -u)
export PROMPT_CONFIG_FILE=${PWD}/src/nvidia_rag/rag_server/prompt.yaml

# ==========================
# Model Cache Directory
# ==========================
export MODEL_DIRECTORY="${HOME}/.cache/nim"

# ==========================
# Docker Volume Directory
# ==========================
export DOCKER_VOLUME_DIRECTORY="${PWD}/volumes"

# ==========================
# RAG Optimization Settings (Option C - Balanced)
# ==========================
# Enable query rewriting for better multi-turn conversations
export ENABLE_QUERYREWRITER=True

# Retrieval depth tuning
export VECTOR_DB_TOPK=150          # Retrieve more candidates (up from 100)
export APP_RETRIEVER_TOPK=15       # More context to LLM (up from 10)

# Filter low-confidence results from reranker
export RERANKER_CONFIDENCE_THRESHOLD=0.3

# NIM LLM Performance Tuning
export NIM_MAX_BATCH_SIZE=32
export NIM_KV_CACHE_FREE_GPU_MEM_FRACTION=0.90
export NIM_ENABLE_CHUNKED_PREFILL=1
export NIM_MAX_CONCURRENT_REQUESTS=64
