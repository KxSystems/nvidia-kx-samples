# ==========================
# KDB.AI Server Configuration
# ==========================
# This file contains environment variables for using KDB.AI as the vector store.
# Reference: https://docs.kx.com/1.7/KDB_AI/Get_Started/kdb-ai-server-setup.htm
#
# Prerequisites:
#   1. Obtain a KDB.AI license from KX (https://kx.com) - valid for 90 days
#   2. Get your Docker registry credentials from the KX welcome email
#   3. Set all required environment variables below
#
# Usage:
#   cp .env.kdbai .env.kdbai.local
#   # Edit .env.kdbai.local with your settings
#   source .env.kdbai.local
#   docker compose --profile kdbai -f vectordb.yaml -f docker-compose-rag-server.yaml up

# ==========================
# KX Docker Registry Authentication (REQUIRED)
# ==========================
# Your KX signup email for Docker registry authentication
export KDBAI_REGISTRY_EMAIL="your-email@example.com"

# Bearer token from your KX welcome email for Docker registry authentication
export KDBAI_REGISTRY_TOKEN="your-bearer-token"

# ==========================
# KDB.AI License (REQUIRED)
# ==========================
# Base64-encoded KDB.AI license string from your KX welcome email
export KDB_LICENSE_B64="your-kdb-license-b64-string"

# ==========================
# KDB.AI Server Settings
# ==========================
# Number of threads for KDB.AI server (Standard Edition limited to 24 cores)
export KDBAI_THREADS=8

# ==========================
# KDB.AI Connection Settings
# ==========================
# For self-hosted KDB.AI Server (Docker Compose deployment)
# The client endpoint is on port 8082 inside the container, mapped to 8084 on host
export KDBAI_ENDPOINT="http://kdbai:8082"

# API key is optional for self-hosted deployments
# Leave empty for local Docker Compose deployment
export KDBAI_API_KEY=""

# Database name within KDB.AI (default: "default")
export KDBAI_DATABASE="default"

# Index type for vector similarity search
# Options: "flat", "hnsw", "ivf", "ivfpq", "cagra" (GPU)
# - flat: Exact search, best for small datasets
# - hnsw: Hierarchical Navigable Small World, good balance of speed/accuracy
# - ivf: Inverted File Index, good for large datasets
# - ivfpq: IVF with Product Quantization, memory efficient for very large datasets
# - cagra: NVIDIA cuVS GPU-accelerated index (recommended for GPU deployments)
export KDBAI_INDEX_TYPE="cagra"

# CAGRA batch insert mode (false = normal batching, fixed in cuVS 1.8.2+)
export KDBAI_CAGRA_SINGLE_BATCH="false"

# ==========================
# KDB.AI GPU Configuration (cuVS)
# ==========================
# Enable these settings when using the KDB.AI cuVS image for GPU-accelerated
# vector indexing and search. Requires the cuVS-enabled Docker image.
#
# To use GPU-accelerated KDB.AI:
#   1. Set KDBAI_GPU_IMAGE to the cuVS image
#   2. Set APP_VECTORSTORE_ENABLEGPUINDEX=True
#   3. Set APP_VECTORSTORE_ENABLEGPUSEARCH=True
#   4. Run: docker compose -f vectordb.yaml --profile kdbai-gpu up -d

# Enable GPU-accelerated indexing (requires cuVS image)
export APP_VECTORSTORE_ENABLEGPUINDEX=True

# Enable GPU-accelerated search (requires cuVS image)
export APP_VECTORSTORE_ENABLEGPUSEARCH=True

# GPU device ID for KDB.AI cuVS (default: 0)
export KDBAI_GPU_DEVICE_ID=0

# KDB.AI cuVS Docker image
export KDBAI_GPU_IMAGE="portal.dl.kx.com/kdbai-db-cuvs:1.8.2"

# ==========================
# Vector Store Configuration
# ==========================
# Set the vector store type to kdbai
export APP_VECTORSTORE_NAME="kdbai"

# Set the vector store URL to your KDB.AI endpoint
# For Docker Compose: use internal container name and port
export APP_VECTORSTORE_URL="http://kdbai:8082"

# Search type: "dense" for vector similarity
# Note: KDB.AI uses distance metrics (L2, CS) for similarity search
export APP_VECTORSTORE_SEARCHTYPE="dense"

# ==========================
# User / Auth (from base .env)
# ==========================
export USERID=$(id -u)
export NVIDIA_API_KEY=${NGC_API_KEY}

# ==========================
# On-Prem NIM Endpoints (from base .env)
# ==========================
export APP_LLM_SERVERURL=nim-llm:8000
export APP_FILTEREXPRESSIONGENERATOR_SERVERURL=nim-llm:8000
export SUMMARY_LLM_SERVERURL=nim-llm:8000
export APP_EMBEDDINGS_SERVERURL=nemoretriever-embedding-ms:8000
export APP_RANKING_SERVERURL=nemoretriever-ranking-ms:8000
export OCR_GRPC_ENDPOINT=paddle:8001
export OCR_INFER_PROTOCOL=grpc
export OCR_MODEL_NAME=paddle
export OCR_HTTP_ENDPOINT=http://paddle:8000/v1/infer
export YOLOX_GRPC_ENDPOINT=page-elements:8001
export YOLOX_INFER_PROTOCOL=grpc
export YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT=graphic-elements:8001
export YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=grpc
export YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT=table-structure:8001
export YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=grpc

# ==========================
# GPU IDs (local deployment)
# ==========================

# ==== LLM ====
export LLM_MS_GPU_ID=1

# ==== Embeddings ====
export EMBEDDING_MS_GPU_ID=0
export VLM_EMBEDDING_MS_GPU_ID=0

# ==== Reranker ====
export RANKING_MS_GPU_ID=0

# ==== Ingestion NIMs GPU ids ====
export YOLOX_MS_GPU_ID=0
export YOLOX_GRAPHICS_MS_GPU_ID=0
export YOLOX_TABLE_MS_GPU_ID=0
export OCR_MS_GPU_ID=0

# ==========================
# Paths
# ==========================
export PROMPT_CONFIG_FILE=${PWD}/src/nvidia_rag/rag_server/prompt.yaml
