# -- Global chart configuration
nameOverride: ""
fullnameOverride: "rag-server"
# subsection: rag-server
# RAG Orchestrator Service
# -- Kubernetes scheduling
nodeSelector: {}
affinity: {}
tolerations: []

# -- Common service account for rag-server
serviceAccount:
  create: true
  name: ""
  automount: true
  annotations: {}

# -- Replicas for rag-server
replicaCount: 1

# -- Namespace for documentation/reference; not actively used in templates
namespace: "nv-nvidia-blueprint-rag"

# -- Image pull secret for all images used by this chart
imagePullSecret:
  name: "ngc-secret"
  registry: "nvcr.io"
  username: "$oauthtoken"
  password: ""
  create: true

# -- Secret containing API keys for NVIDIA NGC model registry
ngcApiSecret:
  name: "ngc-api"
  password: ""
  create: true

# -- RAG server container image
image:
  repository: portal.dl.kx.com/rag-server-kdbai
  tag: "2.3.4"
  pullPolicy: Always

# -- RAG server service configuration
service:
  type: ClusterIP
  port: 8081

# -- RAG server container resources
resources:
  limits:
    memory: "64Gi"
  requests:
    memory: "8Gi"

# -- Probes for rag-server (optional)
livenessProbe:
  httpGet:
    path: /health
    port: 8081
  initialDelaySeconds: 10
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3
readinessProbe:
  httpGet:
    path: /health
    port: 8081
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# -- RAG server runtime configuration
server:
  workers: 8

# -- Enable/disable creation of prompt ConfigMap
promptConfig:
  enabled: true

# -- Environment variables for rag-server
envVars:
  EXAMPLE_PATH: "./nvidia_rag/rag_server"
  PROMPT_CONFIG_FILE: "/prompt.yaml"
  PROMETHEUS_MULTIPROC_DIR: "/tmp-data/prom_data"

  ##===MINIO specific configurations used to store multimodal base64 content===
  MINIO_ENDPOINT: "rag-minio:9000"
  MINIO_ACCESSKEY: "minioadmin"
  MINIO_SECRETKEY: "minioadmin"

  ##===Vector DB specific configurations===
  # URL on which vectorstore is hosted
  # - KDB.AI: "http://kdbai:8082" (default)
  # - Milvus: "http://milvus:19530"
  # - Elasticsearch: "http://elasticsearch:9200"
  APP_VECTORSTORE_URL: "http://kdbai:8082"
  # Type of vectordb used to store embedding supported type "kdbai", "milvus", or "elasticsearch"
  APP_VECTORSTORE_NAME: "kdbai"
  # Index type (e.g., hnsw for KDB.AI, GPU_CAGRA for Milvus)
  APP_VECTORSTORE_INDEXTYPE: "hnsw"
  # Type of vectordb search to be used
  APP_VECTORSTORE_SEARCHTYPE: "dense"
  # Boolean to control GPU search/indexing
  APP_VECTORSTORE_ENABLEGPUSEARCH: "True"
  APP_VECTORSTORE_ENABLEGPUINDEX: "True"
  # ef: Parameter controlling query time/accuracy trade-off. Higher ef leads to more accurate but slower search.
  APP_VECTORSTORE_EF: "100"

  ##===KDB.AI specific configurations===
  # KDB.AI database name
  KDBAI_DATABASE: "default"
  # KDB.AI index type: flat, hnsw, ivf, ivfpq, cagra (GPU)
  KDBAI_INDEX_TYPE: "cagra"
  # KDB.AI API key (optional for self-hosted deployments)
  KDBAI_API_KEY: ""

  # vectorstore collection name to store embeddings
  COLLECTION_NAME: "multimodal_data"
  APP_RETRIEVER_SCORETHRESHOLD: "0.25"
  # Top K from vector DB, which goes as input to reranker model - not applicable if ENABLE_RERANKER is set to False
  VECTOR_DB_TOPK: "100"
  # Number of document chunks to insert in LLM prompt
  APP_RETRIEVER_TOPK: "10"

  ##===LLM Model specific configurations===
  APP_LLM_MODELNAME: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
  # URL on which LLM model is hosted. If "", Nvidia hosted API is used
  APP_LLM_SERVERURL: "nim-llm:8000"
  # LLM model parameters
  LLM_MAX_TOKENS: "32768"
  LLM_TEMPERATURE: "0"
  LLM_TOP_P: "1.0"

  ##===Query Rewriter Model specific configurations===
  APP_QUERYREWRITER_MODELNAME: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
  # URL on which query rewriter model is hosted. If "", Nvidia hosted API is used
  APP_QUERYREWRITER_SERVERURL: "nim-llm:8000"

  ##===Filter Expression Generator Model specific configurations===
  APP_FILTEREXPRESSIONGENERATOR_MODELNAME: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
  # URL on which filter expression generator model is hosted. If "", Nvidia hosted API is used
  APP_FILTEREXPRESSIONGENERATOR_SERVERURL: "nim-llm:8000"
  # enable filter expression generator for natural language to filter expression conversion
  ENABLE_FILTER_GENERATOR: "False"

  ##===Embedding Model specific configurations===
  # URL on which embedding model is hosted. If "", Nvidia hosted API is used
  APP_EMBEDDINGS_SERVERURL: "nemoretriever-embedding-ms:8000"
  APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"

  ##===Reranking Model specific configurations===
  # URL on which ranking model is hosted. If "", Nvidia hosted API is used
  APP_RANKING_SERVERURL: "nemoretriever-ranking-ms:8000"
  APP_RANKING_MODELNAME: "nvidia/llama-3.2-nv-rerankqa-1b-v2"
  ENABLE_RERANKER: "True"
  # Default confidence threshold for filtering documents by reranker relevance scores (0.0 to 1.0)
  RERANKER_CONFIDENCE_THRESHOLD: "0.0"

  ##===VLM Model specific configurations===
  ENABLE_VLM_INFERENCE: "false"
  # Reasoning gate on VLM response
  ENABLE_VLM_RESPONSE_REASONING: "false"
  # Max images sent to VLM per request (query + context)
  APP_VLM_MAX_TOTAL_IMAGES: "4"
  # Use VLM for final response generation
  APP_VLM_RESPONSE_AS_FINAL_ANSWER: "false"
  # Max number of query images to include in VLM input
  APP_VLM_MAX_QUERY_IMAGES: "1"
  # Max number of context images to include in VLM input
  APP_VLM_MAX_CONTEXT_IMAGES: "1"
  APP_VLM_SERVERURL: "http://nim-vlm:8000/v1"
  APP_VLM_MODELNAME: "nvidia/llama-3.1-nemotron-nano-vl-8b-v1"

  # === Text Splitter ===
  APP_TEXTSPLITTER_CHUNKSIZE: "2000"
  APP_TEXTSPLITTER_CHUNKOVERLAP: "200"

  # === General ===
  # Choose whether to enable citations in the response
  ENABLE_CITATIONS: "True"
  # Choose whether to enable/disable guardrails
  ENABLE_GUARDRAILS: "False"
  # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
  LOGLEVEL: "INFO"
  # enable multi-turn conversation in the rag chain - this controls conversation history usage
  # while doing query rewriting and in LLM prompt
  ENABLE_MULTITURN: "True"
  # enable query rewriting for multiturn conversation in the rag chain.
  # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
  ENABLE_QUERYREWRITER: "False"
  # number of last n chat messages to consider from the provided conversation history
  CONVERSATION_HISTORY: "5"

  # === Tracing ===
  APP_TRACING_ENABLED: "False"
  # HTTP endpoint
  APP_TRACING_OTLPHTTPENDPOINT: "http://rag-opentelemetry-collector:4318/v1/traces"
  # GRPC endpoint
  APP_TRACING_OTLPGRPCENDPOINT: "grpc://rag-opentelemetry-collector:4317"

  # === Reflection ===
  # enable reflection (context relevance and response groundedness checking) in the rag chain
  ENABLE_REFLECTION: "false"
  # Maximum number of context relevance loop iterations
  MAX_REFLECTION_LOOP: "3"
  # Minimum relevance score threshold (0-2)
  CONTEXT_RELEVANCE_THRESHOLD: "1"
  # Minimum groundedness score threshold (0-2)
  RESPONSE_GROUNDEDNESS_THRESHOLD: "1"
  # reflection llm
  REFLECTION_LLM: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
  # reflection llm server url. If "", Nvidia hosted API is used
  REFLECTION_LLM_SERVERURL: "nim-llm:8000"

  # Choose whether to enable source metadata in document content during generation
  ENABLE_SOURCE_METADATA: "true"

  # Whether to filter content within <think></think> tags in model responses
  FILTER_THINK_TOKENS: "true"

  NEMO_GUARDRAILS_URL: "nemo-guardrails:7331"

  # enable iterative query decomposition
  ENABLE_QUERY_DECOMPOSITION: "false"
  # maximum recursion depth for iterative query decomposition
  MAX_RECURSION_DEPTH: "3"

# -- Ingestor Server
# subsection: ingestor-server
# Ingestor API Service
ingestor-server:
  enabled: true
  appName: ingestor-server

  # -- Pod scheduling
  nodeSelector: {}
  affinity: {}
  tolerations: []

  replicaCount: 1

  imagePullSecret:
    create: false
    name: "ngc-secret"
    registry: "nvcr.io"
    username: "$oauthtoken"
    password: ""

  image:
    repository: portal.dl.kx.com/ingestor-server-kdbai
    tag: "2.3.4"
    pullPolicy: Always

  # -- Service config for ingestor-server
  service:
    type: ClusterIP
    port: 8082

  server:
    workers: 1

  # -- Probes for ingestor-server (optional)
  livenessProbe: {}
  readinessProbe: {}

  resources:
    limits:
      memory: "25Gi"
    requests:
      memory: "25Gi"

  envVars:
    # Path to example directory relative to repo root
    EXAMPLE_PATH: "src/nvidia_rag/ingestor_server"
    # Absolute path to custom prompt.yaml file
    PROMPT_CONFIG_FILE: "/prompt.yaml"
    # === Vector Store Configurations ===
    # - KDB.AI: "http://kdbai:8082" (default)
    # - Milvus: "http://milvus:19530"
    # - Elasticsearch: "http://elasticsearch:9200"
    APP_VECTORSTORE_URL: "http://kdbai:8082"
    APP_VECTORSTORE_NAME: "kdbai" # supported values: "kdbai", "milvus", or "elasticsearch"
    APP_VECTORSTORE_SEARCHTYPE: "dense"
    APP_VECTORSTORE_ENABLEGPUINDEX: "True"
    APP_VECTORSTORE_ENABLEGPUSEARCH: "True"
    COLLECTION_NAME: "multimodal_data"

    # === KDB.AI specific configurations ===
    KDBAI_DATABASE: "default"
    KDBAI_INDEX_TYPE: "cagra"
    KDBAI_API_KEY: ""

    # === MinIO Configurations ===
    MINIO_ENDPOINT: "rag-minio:9000"
    MINIO_ACCESSKEY: "minioadmin"
    MINIO_SECRETKEY: "minioadmin"

    # === Embeddings Configurations ===
    APP_EMBEDDINGS_SERVERURL: "nemoretriever-embedding-ms:8000"
    APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    APP_EMBEDDINGS_DIMENSIONS: "2048"

    # === NV-Ingest Configurations ===
    APP_NVINGEST_MESSAGECLIENTHOSTNAME: "rag-nv-ingest"
    APP_NVINGEST_MESSAGECLIENTPORT: "7670"

    # === NV-Ingest extraction configurations ===
    APP_NVINGEST_PDFEXTRACTMETHOD: "None"  # Method used for text extraction from "None", "pdfium", "nemoretriever_parse"
    APP_NVINGEST_EXTRACTTEXT: "True"  # Enable text extraction
    APP_NVINGEST_EXTRACTINFOGRAPHICS: "False"  # Enable infographic extraction
    APP_NVINGEST_EXTRACTTABLES: "True"  # Enable table extraction
    APP_NVINGEST_EXTRACTCHARTS: "True"  # Enable chart extraction
    APP_NVINGEST_EXTRACTIMAGES: "False"  # Enable image extraction
    APP_NVINGEST_EXTRACTPAGEASIMAGE: "False"  # Extracts each page as image if enabled
    APP_NVINGEST_STRUCTURED_ELEMENTS_MODALITY: ""  # "image", "text_image"
    APP_NVINGEST_IMAGE_ELEMENTS_MODALITY: ""  # "image"
    APP_NVINGEST_TEXTDEPTH: "page"  # Extract text by "page" or "document"

    # === NV-Ingest caption configurations ===
    APP_NVINGEST_CAPTIONMODELNAME: "nvidia/llama-3.1-nemotron-nano-vl-8b-v1"  # Model name for captioning
    APP_NVINGEST_CAPTIONENDPOINTURL: ""  # Endpoint URL for captioning model

    # === NV-Ingest save to disk configurations ===
    APP_NVINGEST_SAVETODISK: "False"
    NVINGEST_MINIO_BUCKET: "nv-ingest"

    # === General ===
    # Summary Model Configurations
    SUMMARY_LLM: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
    SUMMARY_LLM_SERVERURL: "nim-llm:8000"
    SUMMARY_LLM_MAX_CHUNK_LENGTH: "50000"
    SUMMARY_CHUNK_OVERLAP: "200"

    # === General ===
    ENABLE_CITATIONS: "True"
    LOGLEVEL: "INFO"

    # === NV-Ingest splitting configurations ===
    APP_NVINGEST_CHUNKSIZE: "512"  # Size of chunks for splitting
    APP_NVINGEST_CHUNKOVERLAP: "150"  # Overlap size for chunks
    APP_NVINGEST_ENABLEPDFSPLITTER: "True"  # Enable PDF splitter
    APP_NVINGEST_SEGMENTAUDIO: "False"  # Enable audio segmentation for NV Ingest

    # === Redis configurations ===
    REDIS_HOST: "rag-redis-master"
    REDIS_PORT: "6379"
    REDIS_DB: "0"

    # === Bulk upload to MinIO ===
    ENABLE_MINIO_BULK_UPLOAD: "True"
    TEMP_DIR: "/tmp-data"
    INGESTOR_SERVER_DATA_DIR: "/data/"

    # === NV-Ingest Batch Mode Configurations ===
    NV_INGEST_FILES_PER_BATCH: "16"
    NV_INGEST_CONCURRENT_BATCHES: "4"

  # -- Persistent storage for ingestor-server data directory
  # If enabled, mounts a PVC at the path used by INGESTOR_SERVER_DATA_DIR
  persistence:
    enabled: true
    # If set, use an existing PVC by name; when empty and enabled, a PVC is created
    existingClaim: ""
    # StorageClass to use for the PVC; keep empty to use the cluster default
    storageClass: ""
    # Access modes for the PVC
    accessModes:
      - ReadWriteOnce
    # Requested size for the PVC
    size: 50Gi
    # Mount path inside the container; defaults to envVars.INGESTOR_SERVER_DATA_DIR if empty
    mountPath: "/data/"
    # Optional subPath within the PVC
    subPath: ""

# -- Frontend
# subsection: frontend
# rag frontend Frontend
frontend:
  enabled: true
  appName: "rag-frontend"

  replicaCount: 1

  image:
    repository: nvcr.io/nvidia/blueprint/rag-frontend
    pullPolicy: IfNotPresent
    tag: "2.3.0"

  imagePullSecret:
    name: "ngc-secret"
    registry: "nvcr.io"
    username: "$oauthtoken"
    password: ""

  service:
    type: NodePort
    port: 3000

  # -- Probes for frontend (optional)
  livenessProbe: {}
  readinessProbe: {}

  envVars:
    # Runtime environment variables for Vite frontend
    # Note: Model names are now managed by frontend settings store
    - name: VITE_API_CHAT_URL
      value: "http://rag-server:8081/v1"
    - name: VITE_API_VDB_URL
      value: "http://ingestor-server:8082/v1"
    - name: VITE_MILVUS_URL
      value: "http://milvus:19530"

# -- Elasticsearch dependency toggle
# subsection: elasticsearch
elasticsearch:
  enabled: false
  fullnameOverride: elasticsearch
  security:
    enabled: false
    tls:
      restEncryption: false
  extraConfig:
    discovery.type: single-node
  master:
    masterOnly: false
    replicaCount: 1
  data:
    replicaCount: 0
  coordinating:
    replicaCount: 0
  ingest:
    enabled: false
  global:
    security:
      allowInsecureImages: true
  image:
    registry: docker.io
    repository: bitnamilegacy/elasticsearch
    tag: 9.0.3-debian-12-r1
  volumePermissions:
    image:
      registry: docker.io
      repository: bitnamilegacy/os-shell
      tag: 12-debian-12-r48
  sysctlImage:
    registry: docker.io
    repository: bitnamilegacy/os-shell
    tag: 12-debian-12-r48

# -- KDB.AI Vector Database Configuration (Self-Hosted)
# subsection: kdbai
# Reference: https://docs.kx.com/1.7/KDB_AI/Get_Started/kdb-ai-server-setup.htm
#
# Prerequisites:
#   1. Obtain a KDB.AI license from KX (https://kx.com) - valid for 90 days
#   2. Get your Docker registry credentials from the KX welcome email
#   3. Create the kdbai-credentials secret with your license and registry auth
#
# To enable KDB.AI:
#   1. Set kdbai.enabled: true
#   2. Set elasticsearch.enabled: false (if previously enabled)
#   3. Update nv-ingest.milvusDeployed: false to disable Milvus
#   4. Update APP_VECTORSTORE_NAME: "kdbai" in envVars
#   5. Update APP_VECTORSTORE_URL: "http://kdbai:8082" in envVars
#   6. Create the required secrets (see kdbai-secret.yaml template)
kdbai:
  enabled: false
  fullnameOverride: kdbai

  # -- KDB.AI image configuration (cuVS GPU-accelerated)
  # Requires authentication with KX Docker registry (portal.dl.kx.com)
  image:
    registry: portal.dl.kx.com
    repository: kdbai-db-cuvs
    tag: "1.8.2"
    pullPolicy: IfNotPresent

  # -- Image pull secret for KX Docker registry
  # Create using: kubectl create secret docker-registry kdbai-registry-secret \
  #   --docker-server=portal.dl.kx.com \
  #   --docker-username=<your-email> \
  #   --docker-password=<bearer-token>
  imagePullSecret:
    name: "kdbai-registry-secret"
    # Set to true to create the secret from values below
    create: false
    # Required if create: true
    email: ""
    token: ""

  # -- KDB.AI license secret
  # Create using: kubectl create secret generic kdbai-license-secret \
  #   --from-literal=KDB_LICENSE_B64=<your-base64-license>
  licenseSecret:
    name: "kdbai-license-secret"
    # Set to true to create the secret from values below
    create: false
    # Required if create: true - base64-encoded KDB.AI license string
    licenseB64: ""

  # -- KDB.AI service configuration
  service:
    type: ClusterIP
    # REST API port (health checks, management)
    restPort: 8081
    # Client port (vector operations via kdbai-client SDK)
    clientPort: 8082

  # -- KDB.AI server resources
  resources:
    limits:
      memory: "16Gi"
      cpu: "8"
    requests:
      memory: "4Gi"
      cpu: "2"

  # -- KDB.AI server configuration
  config:
    # Number of threads for KDB.AI server (Standard Edition limited to 24 cores)
    threads: 8
    # Data directory inside the container
    dataDir: "/tmp/kx/data/vdb"

  # -- Persistence configuration for KDB.AI data
  persistence:
    enabled: true
    # StorageClass to use; leave empty to use cluster default
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    size: 50Gi

  # -- Probes for KDB.AI server
  livenessProbe:
    httpGet:
      path: /api/v2/ready
      port: 8081
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
  readinessProbe:
    httpGet:
      path: /api/v2/ready
      port: 8081
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

  # -- Pod scheduling
  nodeSelector: {}
  affinity: {}
  tolerations: []

# -- Standalone MinIO Object Storage
# subsection: minio
# Use this when deploying KDB.AI or Elasticsearch without Milvus.
# MinIO is required for storing multimodal content (images extracted from documents).
# When using Milvus (milvusDeployed: true), MinIO is included as a Milvus sub-dependency,
# so this standalone MinIO should be disabled to avoid conflicts.
minio:
  enabled: false  # Enable when using KDB.AI or Elasticsearch without Milvus
  fullnameOverride: "rag-minio"

  # -- MinIO image configuration (official minio/minio image)
  image:
    repository: minio/minio
    tag: "RELEASE.2025-09-07T16-13-09Z"
    pullPolicy: IfNotPresent

  # -- MinIO authentication
  auth:
    rootUser: "minioadmin"
    rootPassword: "minioadmin"

  # -- Default bucket to create
  defaultBuckets: "nv-ingest"

  # -- Persistence configuration
  persistence:
    enabled: true
    size: 50Gi
    storageClass: ""

  # -- Resource limits
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "2Gi"
      cpu: "1"

  # -- Service configuration
  service:
    type: ClusterIP

# -- Observability
# subsection: serviceMonitor
serviceMonitor:
  enabled: false

# subsection: opentelemetry-collector
opentelemetry-collector:
  enabled: false
  mode: deployment
  image:
    repository: otel/opentelemetry-collector-contrib
    tag: "0.131.0"
  command:
    name: otelcol-contrib
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: '${env:MY_POD_IP}:4317'
          http:
            cors:
              allowed_origins:
                - "*"
    exporters:
      # NOTE: Prior to v0.86.0 use `logging` instead of `debug`.
      zipkin:
        endpoint: "http://rag-zipkin:9411/api/v2/spans"
      debug:
        verbosity: detailed
      prometheus:
        endpoint: ${env:MY_POD_IP}:8889
    extensions:
      health_check: {}
      zpages:
        endpoint: 0.0.0.0:55679
    processors:
      batch: {}
      tail_sampling:
        # filter out health checks
        # https://github.com/open-telemetry/opentelemetry-collector/issues/2310#issuecomment-1268157484
        policies:
          - name: drop_noisy_traces_url
            type: string_attribute
            string_attribute:
              key: http.target
              values:
                - \/health
              enabled_regex_matching: true
              invert_match: true
      transform:
        trace_statements:
          - context: span
            statements:
              - set(status.code, 1) where attributes["http.path"] == "/health"

              # after the http target has been anonymized, replace other aspects of the span
              - replace_match(attributes["http.route"], "/v1", attributes["http.target"]) where attributes["http.target"] != nil

              # replace the title of the span with the route to be more descriptive
              - replace_pattern(name, "/v1", attributes["http.route"]) where attributes["http.route"] != nil

              # set the route to equal the URL if it's nondescriptive (for the embedding case)
              - set(name, Concat([name, attributes["http.url"]], " ")) where name == "POST"
    service:
      extensions: [zpages, health_check]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [debug, zipkin]
          processors: [tail_sampling, transform]
        metrics:
          exporters:
            - debug
            - prometheus
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
            - prometheus
        logs:
          receivers: [otlp]
          exporters: [debug]
          processors: [batch]
  ports:
    metrics:
      enabled: true
      containerPort: 8889
      servicePort: 8889
      protocol: TCP

# subsection: zipkin
zipkin:
  enabled: false

# subsection: kube-prometheus-stack
kube-prometheus-stack:
  enabled: false
  prometheus:
    serviceMonitor:
      interval: "1s"
    prometheusSpec:
      scrapeInterval: "1s"
      evaluationInterval: "1s"
      additionalScrapeConfigs:
        - job_name: rag-app
          scrape_interval: 5s
          metrics_path: /metrics
          static_configs:
            - targets:
                # If the app is in another namespace, use rag-server.<namespace>.svc:8081
                - rag-server:8081
  grafana:
    adminUser: admin
    adminPassword: "admin"

# -- NIMs (dependencies) configuration
# subsection: nim-llm
# NIM LLM
nim-llm:
  enabled: true
  service:
    name: "nim-llm"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.3-nemotron-super-49b-v1.5
    pullPolicy: IfNotPresent
    tag: "1.13.1"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1

  env:
    - name: NIM_MODEL_PROFILE
      value: "" # Provide correct profile name here
  model:
    ngcAPIKey: ""
    name: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
    hfTokenSecret: ""

# subsection: nvidia-nim-llama-32-nv-embedqa-1b-v2
# NIM Text Embedding
nvidia-nim-llama-32-nv-embedqa-1b-v2:
  enabled: true
  service:
    name: "nemoretriever-embedding-ms"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
    tag: "1.10.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# subsection: nvidia-nim-llama-32-nemoretriever-1b-vlm-embed-v1
# NIM VLM Embedding
nvidia-nim-llama-32-nemoretriever-1b-vlm-embed-v1:
  enabled: false
  service:
    name: "nemoretriever-vlm-embedding-ms"
  image:
    repository: nvcr.io/nvidia/nemo-microservices/llama-3.2-nemoretriever-1b-vlm-embed-v1
    tag: "1.7.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# subsection: text-reranking-nim
# NIM Text Reranking
nvidia-nim-llama-32-nv-rerankqa-1b-v2:
  enabled: true
  service:
    name: "nemoretriever-ranking-ms"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2
    tag: "1.8.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# subsection: nim-vlm
# NIM Vision-Language (VLM)
nim-vlm:
  enabled: false
  service:
    name: "nim-vlm"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.1-nemotron-nano-vl-8b-v1
    tag: "1.3.1"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# -- NV-Ingest dependency configuration
# subsection: nv-ingest
# NV-Ingest Service
nv-ingest:
  enabled: true
  logLevel: WARNING
  imagePullSecrets:
    - name: "ngc-secret"
  ngcApiSecret:
    create: false
  ngcImagePullSecret:
    create: false
  image:
    repository: "nvcr.io/nvidia/nemo-microservices/nv-ingest"
    tag: "25.9.0"
  resources:
    limits:
      nvidia.com/gpu: 0
  envVars:
    ARROW_DEFAULT_MEMORY_POOL: "system"
    # IMPORTANT: Set to null to override subchart default and avoid duplicate env var error
    # The template already handles INGEST_LOG_LEVEL explicitly, use logLevel instead
    INGEST_LOG_LEVEL: null
    INGEST_RAY_LOG_LEVEL: "PRODUCTION"
    NV_INGEST_MAX_UTIL: 48
    INGEST_EDGE_BUFFER_SIZE: 64
    INGEST_DYNAMIC_MEMORY_THRESHOLD: "0.8"
    INGEST_DISABLE_DYNAMIC_SCALING: "false"
    MRC_IGNORE_NUMA_CHECK: 1
    READY_CHECK_ALL_COMPONENTS: "False"
    COMPONENTS_TO_READY_CHECK: "ALL"
    REDIS_MORPHEUS_TASK_QUEUE: morpheus_task_queue
    REDIS_INGEST_TASK_QUEUE: "ingest_task_queue"
    NV_INGEST_DEFAULT_TIMEOUT_MS: "1234"
    MAX_INGEST_PROCESS_WORKERS: 16
    EMBEDDING_NIM_ENDPOINT: "http://nemoretriever-embedding-ms:8000/v1"
    EMBEDDING_NIM_MODEL_NAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    MESSAGE_CLIENT_HOST: "rag-redis-master"
    MESSAGE_CLIENT_PORT: 6379
    MESSAGE_CLIENT_TYPE: "redis"
    MINIO_INTERNAL_ADDRESS: "rag-minio:9000"
    MINIO_PUBLIC_ADDRESS: "http://localhost:9000"
    MINIO_BUCKET: "nv-ingest"
    MILVUS_ENDPOINT: "http://milvus:19530"
    #OTEL_EXPORTER_OTLP_ENDPOINT: "otel-collector:4317"
    MODEL_PREDOWNLOAD_PATH: "/workspace/models/"
    INSTALL_AUDIO_EXTRACTION_DEPS: "true"

    # Paddle OCR endpoints
    PADDLE_GRPC_ENDPOINT: nv-ingest-ocr:8001
    PADDLE_HTTP_ENDPOINT: http://nv-ingest-ocr:8000/v1/infer
    PADDLE_INFER_PROTOCOL: grpc

    # OCR routing (defaults to Paddle OCR service)
    OCR_GRPC_ENDPOINT: nv-ingest-ocr:8001
    OCR_HTTP_ENDPOINT: http://nv-ingest-ocr:8000/v1/infer
    OCR_INFER_PROTOCOL: grpc
    OCR_MODEL_NAME: paddle

    # NeMo Retriever Parse (VLM text extraction)
    NEMORETRIEVER_PARSE_HTTP_ENDPOINT: http://nim-vlm-text-extraction-nemoretriever-parse:8000/v1/chat/completions
    NEMORETRIEVER_PARSE_INFER_PROTOCOL: http
    NEMORETRIEVER_PARSE_MODEL_NAME: nvidia/nemoretriever-parse

    # YOLOX endpoints
    YOLOX_GRPC_ENDPOINT: nemoretriever-page-elements-v2:8001
    YOLOX_HTTP_ENDPOINT: http://nemoretriever-page-elements-v2:8000/v1/infer
    YOLOX_INFER_PROTOCOL: grpc
    YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT: nemoretriever-graphic-elements-v1:8001
    YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT: http://nemoretriever-graphic-elements-v1:8000/v1/infer
    YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL: grpc
    YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT: nemoretriever-table-structure-v1:8001
    YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT: http://nemoretriever-table-structure-v1:8000/v1/infer
    YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL: grpc

    # Captioning
    VLM_CAPTION_MODEL_NAME: nvidia/llama-3.1-nemotron-nano-vl-8b-v1
    #VLM_CAPTION_ENDPOINT: http://nim-vlm:8000/v1/chat/completions

    # Audio service
    AUDIO_GRPC_ENDPOINT: nv-ingest-riva-nim:50051
    AUDIO_INFER_PROTOCOL: grpc

  # Expose internal Milvus/MinIO config managed by nv-ingest subchart
  milvusDeployed: true
  milvus:
    image:
      all:
        repository: milvusdb/milvus
        tag: v2.5.17-gpu
    etcd:
      image:
        repository: "milvusdb/etcd"
        tag: "3.5.22-r1"
    standalone:
      resources:
        limits:
          nvidia.com/gpu: 1
    minio:
      image:
        repository: minio/minio
        tag: "RELEASE.2025-09-07T16-13-09Z"
      accessKey: minioadmin
      secretKey: minioadmin
      bucketName: nv-ingest
    fullnameOverride: milvus

  # Redis Master
  redis:
    image:
      repository: redis
      tag: 8.2.1


  # Ensure nv-ingest does not deploy its own embedding NIM
  nvidia-nim-llama-32-nv-embedqa-1b-v2:
    deployed: false

  # Ensure nv-ingest does not deploy its own observability components
  otelDeployed: false
  zipkinDeployed: false

  # Sub-NIMs deployed by NV-Ingest
  # NIM OCR (PaddleOCR)
  paddleocr-nim:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/baidu/paddleocr
      tag: "1.5.0"
    imagePullSecrets:
      - name: ngc-secret
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_RATE_LIMIT
        value: "3"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "3072"
      - name: OMP_NUM_THREADS
        value: "8"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # Nemo Retriever OCR
  nemoretriever-ocr:
    deployed: false
    replicaCount: 1
    image:
      repository: nvcr.io/nvidia/nemo-microservices/nemoretriever-ocr-v1
      tag: "1.1.0"
    imagePullSecrets:
      - name: ngc-secret
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: NIM_TRITON_RATE_LIMIT
        value: "1"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_MAX_BATCH_SIZE
        value: "32"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "2048"
      - name: NIM_TRITON_CPU_THREADS_PRE_PROCESSOR
        value: "2"
      - name: NIM_TRITON_CPU_THREADS_POST_PROCESSOR
        value: "1"
      - name: OMP_NUM_THREADS
        value: "1"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # NIM Graphic Elements
  nemoretriever-graphic-elements-v1:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1
      tag: "1.5.0"
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: NIM_TRITON_RATE_LIMIT
        value: "3"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_MAX_BATCH_SIZE
        value: "32"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "2048"
      - name: OMP_NUM_THREADS
        value: "1"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # NIM Page Elements
  nemoretriever-page-elements-v2:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-page-elements-v2
      tag: "1.5.0"
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: NIM_TRITON_RATE_LIMIT
        value: "3"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_MAX_BATCH_SIZE
        value: "32"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "2048"
      - name: NIM_TRITON_CPU_THREADS_PRE_PROCESSOR
        value: "2"
      - name: NIM_TRITON_CPU_THREADS_POST_PROCESSOR
        value: "1"
      - name: OMP_NUM_THREADS
        value: "2"
      - name: NIM_ENABLE_OTEL
        value: "true"
      - name: NIM_OTEL_SERVICE_NAME
        value: "page-elements"
      - name: NIM_OTEL_TRACES_EXPORTER
        value: "otlp"
      - name: NIM_OTEL_METRICS_EXPORTER
        value: "console"
      - name: NIM_OTEL_EXPORTER_OTLP_ENDPOINT
        value: "http://otel-collector:4318"
      - name: TRITON_OTEL_URL
        value: "http://otel-collector:4318/v1/traces"
      - name: TRITON_OTEL_RATE
        value: "1"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # NIM Table Structure
  nemoretriever-table-structure-v1:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-table-structure-v1
      tag: "1.5.0"
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: NIM_TRITON_RATE_LIMIT
        value: "3"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_MAX_BATCH_SIZE
        value: "32"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "2048"
      - name: OMP_NUM_THREADS
        value: "1"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # NIM VLM Text Extraction
  nim-vlm-text-extraction:
    deployed: false
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-parse
      tag: "1.2"
